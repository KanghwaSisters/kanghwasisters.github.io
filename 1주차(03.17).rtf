{\rtf1\ansi\ansicpg949\cocoartf2821
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red214\green85\blue98;\red0\green0\blue0;\red251\green2\blue7;
\red98\green147\blue254;\red81\green156\blue233;\red98\green147\blue254;\red127\green127\blue127;\red251\green2\blue255;
\red251\green2\blue255;\red127\green127\blue127;}
{\*\expandedcolortbl;;\cssrgb\c87843\c42353\c45882;\cssrgb\c0\c0\c0;\cssrgb\c100000\c14913\c0;
\cssrgb\c45506\c65578\c99793;\cssrgb\c38039\c68235\c93333;\cssrgb\c45506\c65578\c99793;\cssrgb\c57046\c57047\c57046;\cssrgb\c100000\c25279\c100000;
\cssrgb\c100000\c25279\c100000;\cssrgb\c57046\c57047\c57046;}
\paperw11900\paperh16840\margl1440\margr1440\vieww13360\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs26 \cf2 \expnd0\expndtw0\kerning0
# \uc0\u54028 \u51060 \u50028 \u44284  \u52992 \u46972 \u49828 \u47196  \u48176 \u50864 \u45716  \u44053 \u54868 \u54617 \u49845 \cf0 \
\
\cf2 ## 1\uc0\u51452 \u52264  (03.17)\cf0 \
\
\cf2 ### 1. \uc0\u51064 \u44277 \u49888 \u44221 \u47581  \u44060 \u45392 \cf0 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \'97\cf0 \
\
\pard\pardeftab720\partightenfactor0
\cf5 - ** \uc0\u45432 \u46300 \u50752  \u54868 \u49332 \u54364  **\cf0 \
	\cf6 1.\cf0  \uc0\u51064 \u44277 \u49888 \u44221 \u47581 \u51032  \u44228 \u52789 \u44396 \u51312 : \u51077 \u47141 \u52789 , \u51008 \u45769 \u52789 , \u52636 \u47141 \u52789 , \u54637 \u49345  \u51060  \u49692 \u49436 \u47196  \u51221 \u48372 \u44032  \u55120 \u47480 \u45796 .\
		- \uc0\u49900 \u52789 \u49888 \u44221 \u47581 : \u51008 \u45769 \u52789 \u51060  2\u44060  \u51060 \u49345 \u51064  \u51064 \u44277 \u49888 \u44221 \u47581 \
		- Layer: \uc0\u47784 \u46304  \u51064 \u44277 \u49888 \u44221 \u47581  \u47784 \u45944 \u51032  \u44592 \u48376  \u45800 \u50948 \
			- Fully-connected layer: \uc0\u44032 \u51109  \u44036 \u45800 \u54620  \u52789  (Dense\u46972 \u45716  \u47784 \u46280 \u51032  \u53364 \u47000 \u49828  \u49440 \u50616 \u51004 \u47196  \u51060 \u50857 )\
	\cf7 2\cf6 .\cf0  \uc0\u54868 \u49332 \u54364 : *\u49884 \u45253 \u49828 *\u51032  \u50669 \u54624 , \u50672 \u44208 \u51032  \u48708 \u51473 \u51012  \u51312 \u51208 \u54620 \u45796 .\
		- \uc0\u47564 \u50557  \u50672 \u44208 \u51012  \u45796 \u51473 \u51004 \u47196  \u48155 \u51004 \u47732  \u44033 \u44033 \u51012  \u45796 \u47480  \u48708 \u51473 \u51004 \u47196  \u49373 \u44033 \u54620 \u45796 .\
	\cf7 3.\cf0  \uc0\u45432 \u46300 : *\u45684 \u47088 *\u51032  \u50669 \u54624 \
	\cf6 4.\cf0  \uc0\u44592 \u48376  \u44396 \u51312 : \u51077 \u47141  -(\u44032 \u51473 \u52824 )-> \u54200 \u54693  -> \u54876 \u49457 \u54632 \u49688  -> \u52636 \u47141 \
		- \uc0\u54876 \u49457 \u54632 \u49688 \
			- ReLU, Tanh, Sigmoid \uc0\u46321 \u51012  \u51060 \u50857  (\u45800 \u49692 \u55176  switch\u52376 \u47100  0, 1\u47196  \u54876 \u49457 \u54868 \u54616 \u51648  \u50506 \u45716 \u45796 .)\
				- Sigmoid: 0~1 \uc0\u50672 \u49549 \u44050 \u51012  return, 0 or 1\u47196  \u54876 \u49457 \u49345 \u53468 \u47484  \u45208 \u53440 \u45236 \u45716  \u44172  \u50500 \u45768 \u45796 .\
				$$\
				f(x) = frac\{1\}\{1+e^\{-x\}\}\
				$$\
				- ReLU (Rectified Linear Unit): \uc0\u51077 \u47141 \u51060  0\u48372 \u45796  \u53356 \u47732  \u44536 \u45824 \u47196 , \u50500 \u45768 \u47732  0\u51004 \u47196  return\
				- \uc0\u49440 \u54805  V.S. \u48708 \u49440 \u54805 \u54632 \u49688 : \u47928 \u51228 \u44032  \u48373 \u51105 \u54644  \u49440 \u54805 \u51004 \u47196  \u44396 \u48516 \u51060  \u48520 \u44032 \u45733 \u54620  \u44221 \u50864 \u44032  \u47566 \u45796 .\
				(\uc0\u45824 \u48512 \u48516 \u51032  \u49888 \u44221 \u47581 \u51060  \u47120 \u47336 \u47484  \u49324 \u50857 \u54616 \u44256  \u51080 \u45796 .)\
			- \uc0\u54876 \u49457 \u54632 \u49688 \u51032  \u51077 \u47141  = sum of \{(\u51077 \u47141 )*(\u44032 \u51473 \u52824 )+(\u54200 \u54693 )\}\
\
\cf7 - ** \uc0\u46373 \u47084 \u45789  **\cf0 \
	\cf7 1.\cf0  \uc0\u46373 \u47084 \u45789 \u51060 \u46976 ?\
	- \uc0\u46373 \u47084 \u45789  \u51060 \u51204 \u51032  \u44592 \u44228 \u54617 \u49845 \u50640 \u49436 \u45716  \u51204 \u47928 \u44032 \u44032  \u51649 \u51217  \u53945 \u51669 \u51012  \u52628 \u52636 \u54616 \u44256  \u51060 \u47484  \u50044 \u51004 \u45208 , \u51060 \u45716  \u44060 \u48156 , \u54217 \u44032 , \u48372 \u50756 \u51201  \u52769 \u47732 \u50640 \u49436  \u48512 \u51221 \u51201 \u51060 \u45796 .\
	- \uc0\u46373 \u47084 \u45789 \u51008  \u46373 \u47084 \u45789  \u50508 \u44256 \u47532 \u51608 \u51060  \u51077 \u47141  \u53945 \u51669 \u51012  \u52628 \u52636 \u54620 \u45796 . (Better)\
	\cf7 2.\cf0  \uc0\u54617 \u49845 \
	- \uc0\u44032 \u51473 \u52824 \u50752  \u54200 \u54693 \u51012  \u54617 \u49845 \u54616 \u45716  \u44163 \
	- \uc0\u51648 \u46020 \u54617 \u49845 :\
		- \uc0\u54617 \u49845  \u45936 \u51060 \u53552 : \u51077 \u47141  + \u51221 \u45813 (\u53440 \u44611 )\
		- \uc0\u52636 \u47141 : \u50696 \u52769 \u44050 \
		- \uc0\u50724 \u52264 \u54632 \u49688 : \u53440 \u44611 \u44284  \u50696 \u52769 \u51032  \u50724 \u52264  \u44228 \u49328 \
			- ex. \uc0\u54217 \u44512 \u51228 \u44273 \u50724 \u52264 (MSE): $ ** \u50724 \u52264  ** = (\u53440 \u44611  - \u50696 \u52769 )^2 $ -> \u44032 \u51473 \u52824 /\u54200 \u54693 \u51012  \u50629 \u45936 \u51060 \u53944 \u54644 \u49436  \u50724 \u52264  \u51312 \u51221  -> \u50741 \u54000 \u47560 \u51060 \u51200 \u44032  \u47784 \u45944  \u50629 \u45936 \u51060 \u53944 \
		- \uc0\u50669 \u51204 \u54028  (Back-propagation) / \u44221 \u49324 \u54616 \u44053 \u48277 : \u50724 \u52264  \u54632 \u49688 \u47484  \u53685 \u54644  \u44032 \u51473 \u52824 /\u54200 \u54693 \u51032  \u50724 \u52264 \u50640  \u45824 \u54620  \u44592 \u50668 \u46020 \u47484  \u44228 \u49328 \u54620 \u45796 .\
		$$\
		** \uc0\u50629 \u45936 \u51060 \u53944  \u44050  ** \\propto \{\u50724 \u52264 \} * \{\u50724 \u52264  \u44592 \u50668 \u46020 \}\
		$$\
		- \uc0\u50724 \u52264  \u44592 \u50668 \u46020  = \u54644 \u45817  \u50724 \u52264 \u50640  \u45824 \u54620  \u54200 \u48120 \u48516 \u44050  => \u51613 \u44048  \u50668 \u48512 /\u53356 \u44592  \u44208 \u51221 \
			- \uc0\u48120 \u48516 \u44050 \u51060 \u44592 \u50640  \u44221 \u49324 \u46972 \u44256  \u48512 \u47476 \u44592 \u46020  \u54620 \u45796 .\
		- \uc0\u47784 \u46304  \u44221 \u49324 \u54616 \u44053 \u48277 (SGD, RMSprop, Adam \u46321 )\u51008  \u54617 \u49845  \u49549 \u46020 (rate)\u46972 \u45716  \u48320 \u49688 \u47484  \u44054 \u45716 \u45796 .\
			- \uc0\u54617 \u49845  \u49549 \u46020 : \u49888 \u44221 \u47581  \u50629 \u45936 \u51060 \u53944 \u49884  \u50629 \u45936 \u51060 \u53944  \u53356 \u44592  \u44208 \u51221 \
\
\cf7 - ** \uc0\u51064 \u44277 \u49888 \u44221 \u47581  \u46972 \u51060 \u48652 \u47084 \u47532  (\u52992 \u46972 \u49828 ) **\cf0 \
 - \uc0\u51064 \u44277 \u49888 \u44221 \u47581  \u54532 \u47112 \u51076 \u50892 \u53356 : \u51060 \u48120  \u51064 \u44277 \u49888 \u44221 \u47581 \u51012  \u44396 \u54788  -> \u50668 \u44592 \u49436  \u48156 \u51204 \u49884 \u53020  \u49324 \u50857 \
\
\pard\pardeftab720\partightenfactor0
\cf2 ___\cf0 \
\
\cf2 ### \uc0\u51064 \u44277 \u49888 \u44221 \u47581  \u50696 \u51228 \cf4 \
\pard\pardeftab720\partightenfactor0
\cf0 \
import tensorflow as tf\
from tensorflow.keras.layers import Dense\
from tensorflow.keras.optimizers import Adam\
\
\pard\pardeftab720\partightenfactor0
\cf8 # load data\cf0 \
mnist = tf.keras.datasets.fashion_mnist\
(x_train, y_train), (x_test, y_test) = mnist.load_data()\
\
\cf8 # normalize pixels from 0~255 to 0~1\cf0 \
x_train, x_test = x_train / 255.0, x_test / 255.0\
\
\cf8 # Definition of Model\cf0 \
\cf9 \'91\'92\'92\cf0 \
\cf9 - \uc0\u51077 \u47141 \u52789  \u45432 \u46300  \u44060 \u49688 : ** 784 **, \u51008 \u45769 \u52789 : ** 256 **, ** 128 **, \u52636 \u47141 \u52789 : ** 10 **\
- \uc0\u51077 \u47141  * (28,28) * 2\u52264 \u50896  \u48176 \u50676 \u51060  1\u52264 \u50896  \u48176 \u50676  * (784,) *\u47196  \u45720 \u47140  ReLU \u54632 \u49688 \u47484  \u44144 \u52824 \u45796 \u44032  \u52636 \u47141 \u52789 \u50640 \u49436 \u45716  Softmax \u54632 \u49688 \u47484  \u44144 \u52828 \u45796 .\
	- -1 \uc0\u54028 \u46972 \u48120 \u53552 \u47196  \u54644 \u45817  \u52264 \u50896 \u51012  \u51088 \u46041 \u51004 \u47196  \u44228 \u49328 \
	- return = \uc0\u54633 \u51060  1\u51064  \u54869 \u47456  10\u44060  -> \u44033  \u47112 \u51060 \u48660 \u50640  \u45824 \u54620  \u50696 \u52769  \u54869 \u47456 \u51012  \u51032 \u48120 \
- tf.keras.Model\uc0\u51012  \u49345 \u49549 \u48155 \u45716  \u53364 \u47000 \u49828  \u49440 \u50616  \u54980  \u52488 \u44592 \u54868  \u54632 \u49688 \u47196  \u48320 \u54805 \cf0 \
\cf9 \'91\'92\'92\cf0 \
class Model(tf.keras.Model):\
	def __init__(self):\
		super(MOdel, self).__init__()\
		self.input_layer = Dense(256, activation=\'91relu\'92, input_shape(784,))\
		self.hidden_layer = Dense(128, activation=\'91relu\'92)\
		self.output_layer = Dense(10, activation=\'91softmax\'92)\
	\
	def call(self, x):\
		x = self.input_layer(x)\
		x = self.hidden_layer(x)\
		x = self.output_layer(x)\
		return x\
\
\cf8 # Model, Loss, Optimizer\cf0 \
\cf9 \'91\'92\'92\cf0 \
\cf9 - \uc0\u45936 \u51060 \u53552 \u49483 \u51008  \u51060 \u51228  \u51089 \u51008  \u45800 \u50948 \u51064  ** \u48120 \u45768  \u48176 \u52824  **\u47196  \u51900 \u44060 \u51256 \u49436  \u54617 \u49845 \u50640  \u50416 \u51068  \u44163 \u51060 \u45796 .\
	- \uc0\u48176 \u52824 : \u45936 \u51060 \u53552  \u49483 \u51012  \u51900 \u44064  \u44163  * cf. \u48120 \u45768  \u48176 \u52824 : \u48176 \u52824 \u47484  \u51900 \u44064  \u44163  *\
	- why? \uc0\u47784 \u46304  \u45936 \u51060 \u53552 \u50640  \u45824 \u54644  \u50724 \u52264 \u54632 \u49688 \u47484  \u46028 \u47532 \u45716  \u44163 \u51008  \u51088 \u50896  \u45229 \u48708 \
- \uc0\u50724 \u52264 \u54632 \u49688  Cross Entropy: (\u45208 \u51473 \u50640 )\
- \uc0\u50741 \u54000 \u47560 \u51060 \u51200  Adam\
- epoch: \uc0\u51204 \u52404  \u45936 \u51060 \u53552  \u54617 \u49845  \u49688 \u54665  \u54943 \u49688 \
\'91\'92\'92\cf0 \
model = CNN()\
cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\
optimizer = Adam(1e-4)\
\
\cf8 # Mini batch\cf0 \
batch_size = 32\
num_train_data = x_train-shape[0]\
num_test_data = x_test.shape[0]\
\
num_epoch = 10\
for e in range(num_epoch):\
	for i in range(num_train_data//batch_size): \cf8 # \uc0\u48176 \u52824  \u54616 \u45208  \u47564 \u53372 \u50473  \u46028 \u50500 \cf0 \
		\cf8 # \uc0\u48120 \u45768  \u48176 \u52824  \u44060 \u49688  \u47564 \u53372  \u45936 \u51060 \u53552  fetch\cf0 \
		x_batch = x_train[i * batch_size : (i+1) * batch_size]\
		y_batch = y_train[i * batch_size : (i+1) * batch_size]\
		\cf8 # \uc0\u51204 \u52376 \u47532 : 2\u52264 \u50896  \u48176 \u50676  -> 1\u52264 \u50896 , \u49828 \u52860 \u46972  \u44050 \u51008  One-hot encoding\u51004 \u47196  \u48289 \u53552 \u54868 \cf0 \
		x_batch = x_batch.reshape(-1, 28 * 28)\
		y_batch = tf.one_hot(y_batch, 10)\
\
\pard\pardeftab720\partightenfactor0
\cf10 \'91\'92\'92\
tf.GradientTape (): \uc0\u50669 \u51204 \u54028  \u50508 \u44256 \u47532 \u51608 \u51032  \u54952 \u44284 \u51201 \u51064  \u49688 \u54665 \u51012  \u50948 \u54644  \u47784 \u45944  \u51077 \u47141 ~\u52636 \u47141  \u44228 \u49328  \u44284 \u51221 \u51012  \u51200 \u51109 \u54616 \u45716  \u54632 \u49688 \
	-> with \uc0\u47928  \u50504 \u50640 \u49436  \u51068 \u50612 \u45212  \u44228 \u49328 \u51012  \u47784 \u46160  tape\u50640  \u51200 \u51109 \u54620 \u45796 .\
	-> \uc0\u51200 \u51109 \u46108  \u44050 \u51004 \u47196  tape.gradient() \u44396 \u54620 \u45796 . (= \u54200 \u48120 \u48516  \u44050 \u51012  \u44396 \u54620 \u45796 .)\
\'91\'92\'92\
		\cf11 # \uc0\u44228 \u49328  \u44284 \u51221  \u44592 \u47197 \u50857  tape scope \u49440 \u50616 \cf0 \
		model_params = model.trainable_variables\
		with tf.GradientTape() as tape:\
			\cf11 # \uc0\u47784 \u45944 \u47196  \u50696 \u52769 \cf0 \
			predicts = model(x_batch)\
			\cf11 # \uc0\u50724 \u47448 \u54632 \u49688  \u44228 \u49328 \cf0 \
			losses = cross_entropy(predicts, y_batch)\
		\cf11 # tape\uc0\u47484  \u53685 \u54644  gradient \u44228 \u49328 \cf0 \
		grads = tape.gradient(losses, model_params)\
		\cf11 # \uc0\u44228 \u49328 \u54620  \u44536 \u47112 \u51060 \u46356 \u50616 \u53944 \u47484  \u53685 \u54644  \u47784 \u45944  \u50629 \u45936 \u51060 \u53944 \cf0 \
		optimizer.apply_gradients(zip(grads, model_params))\
\
\pard\pardeftab720\partightenfactor0
\cf2 ___\
\
### \uc0\u44284 \u51228 \cf0 \
\
- \uc0\u54876 \u49457 \u54868  \u54632 \u49688  \u49440 \u53469 \u49884 : ReLU, Tanh, Sigmoid\u51032  \u54632 \u49688  \u49440 \u53469  \u44592 \u51456 \u51060  \u47924 \u50631 \u51064 \u44032 \u50836 ?\
- SGD, RMSprop, Adam \uc0\u46321 \u51032  \u44221 \u49324 \u54616 \u44053 \u48277  \u51473  \u50741 \u54000 \u47560 \u51060 \u51200 \u47484  \u49440 \u53469 \u54616 \u45716  \u44592 \u51456 \u51060  \u51080 \u45208 \u50836 ?\
- \uc0\u48176 \u52824 \u46993  \u44060 \u45392 \u51201 \u51004 \u47196  \u53356 \u44172  \u45804 \u46972 \u48372 \u51060 \u51648  \u50506 \u45716 \u45936  \uc0\u48120 \u45768  \u48176 \u52824 \u47196  \u51900 \u44060 \u45716  \u51060 \u50976 \u45716  \u47924 \u50631 \u51064 \u44032 \u50836 ?}