---
title: "[04주차/강화학습 세션] 강화학습 방법론"
excerpt: "몬테카를로 제어, 시간차 예측, SARSA, Q-러닝" # 미리보기로 보이는 부분
categories: 24-1강화학습세션
toc: true
toc_sticky: true
comments: true
author: Jimin Lee

date: 2024-03-25

---

# 4주차 강화학습 세션

## 요약
- 저번주 공부한 코드와 내용에 대한 짧은 질의응답 시간을 가졌습니다. 이론 부분의 경우, 1주일 동안 팀채팅을 통해 해결한 문제가 많았기에 구현 및 표현법에 대한 질문이 많았습니다. 
- 개인 발표 주제는 부트스트랩이었고, 이승연 벗께서 발표를 진행해주셨습니다. 
- 메인 세션에서는 model-free한 환경에서 가치를 구하는 여러 강화학습 방법론에 대해 공부했습니다. 
저번 주에 학습한 다이나믹 프로그래밍의 한계부터 시작해 각 강화학습 방법론이 갖고 있는 특징과 한계점에 대해 이야기 나눴습니다. 
- 저번 세션과는 달리, 이번 세션에서는 50분 가량의 시간을 갖고 직접 코드를 구현하는 시간을 가졌습니다. 😎 다이나믹 프로그래밍에서 table type으로만 코드를 짜다보니 dict 형식으로 가치 테이블을 표현하는 것이 까다로웠지만, 서로 서로 내용을 설명해주며 문제를 해결했습니다. 



- 구체적인 개인 발표 내용은 추후 **[개념정리] 카테고리**에서 확인하실 수 있습니다.  

## 개인 발표

- 📗 **부트스트랩** : 이승연 벗

부트스트랩이란 주제를 가지고 통계학, 머신러닝, 강화학습에서 부트스트랩 방법론이 어떻게 사용되는지 설명해주셨습니다. 


## 강화학습 세션

- 📗 **강화학습 : 몬테카를로 예측, 시간차 예측, SARSA, Q-Learning**
- 👩‍💻 **SARSA Agent 코드 구현**

MDP 문제를 해결하기 위해 고안된 강화학습의 근본 방법론에 대해 알아보았습니다. GPI와 강화학습, 몬테카를로 예측, 시간차 예측, 시간차 제어의 SARSA, Q-Learning, 온폴리쉬-오프폴리쉬, 탐험과 같은 키워드들을 중심으로 학습했습니다. 

코드 실습에서는 SARSA Agent를 직접 구현해보는 시간을 가졌습니다. 이를 위해 강화학습의 기본 구조인 Agent-Env의 관계와 강화학습의 주요 구성요소들이 어느 class에 들어가야 하는지 틀을 세웠습니다. 

## 주요 질의응답
**1. step size의 역할이 정확히 무엇인지 궁금하다.** 
- 강화학습에서는 상태-행동 쌍의 가치가 단기적인 학습의 목표가 된다. step size(alpha)는 이 목표값과 현재 가치 사이의 오차를 얼마나 업데이트에 반영할 것인지를 알려주는 지표로, 딥러닝의 learning rate와 유사하다. 

