---
title: "[강화시스터즈 1기/10주차/프로젝트] 지뢰찾기 agent, env 구현"
tags: [24-1]
pages: papers
style: border  # fill / border 
color: dark
description: " "
---
# 10주차 - 01 세션

## 요약 
팀끼리 모여 지뢰찾기 agent, env를 구현했습니다.  
interval print에 어떤 정보가 들어가야 하는지 고민하고, 이미 까진 타일을 계속 누르는 문제를 해결하기 위해 고민했습니다. 

## 👩‍💻 팀 프로젝트 
- **팀구성**   

| 지뢰마스터즈 | Agent | Environment |
| --- |-----|-------------|
| 1팀 | 주민서 | 김도희         |
| 2팀 | 이정연 | 손주현         |

| AI 폭탄 제거 부대 | Agent | Environment |
| --- |-------|-------------|
| 1팀  | 변지은   | 이승연         |
| 2팀 | 이은나   | 김정은         |

### Interval Print
- **지뢰마스터즈**
  - 1팀 : 10000 에피소드 단위로, 50 주기 에피소드의 이동 횟수, 50 주기 에피소드의 epi reward  
  - 2팀 : 1000 에피소드 단위로, 평균 epi reward, current epsilon, replay memory length
- **AI 폭탄 제거 부대**  
  - 1팀 : 10000 에피소드 단위로, 에피소드 10000번의 총 action result
  - 2팀 : 1000 에피소드 단위로, 10개 에피소드의 평균 reward 값  

### 개선 방안 
딥러닝은 acc와 loss를 통해 학습이 되고 있는지 확인한다. 
하지만 강화학습은 target 신경망에서 trg 값을 뽑아내며 일부 값만 수정하기에, loss가 매우 작다. 물론 loss의 수렴은 강화학습에서도 학습이 되고 있는지를 파악하는 지표가 되지만, 딥러닝처럼 학습 현황을 알아볼 수 있는 지표가 되지 못한다. 
따라서 강화학습을 구현할 때는 loss 이외의 학습평가 지표가 필요하다. 
지뢰찾기 테스크에서는 한 에피소드마다 몇 번 클릭되었는지, 에피소드 별 총 보상이 얼마인지, 이 게임을 이겼는지 졌는지가 학습의 지표가 될 수 있다. 
또한 특정 n개의 에피소드 단위로 출력을 하는 것은 그간의 상황을 보여주기 위함이기에 출력 구간의 정보를 모두 포함할 수 있는 지표(ex mean)를 사용해 나타내는 것이 좋다. 
또한 강화학습에서는 하이퍼파라미터 튜닝이 중요하기에,replay memory가 차는 정도나, 현재의 epsilon 값을 시각화시키는 것이 좋다. 

### 이미 까진 타일을 또 누르는 경우
**@ 지뢰마스터즈 01팀, AI 폭탄 제거 부대 01팀**   
- **문제 상황** : 두 팀에게서 이미 까진 타일을 또 눌러 에피소드가 끝나지 않는 문제 발생
- **해결 방안** : 
  1. Agent `get_action()` method에 룰을 추가한다. 
     - 어떻게 구현하나요? -> net에서 Q-val을 받은 후, 이미 깐 타일 위치의 Q값을 `min(Q_value)`로 맞춰준다.
     - 0으로 맞추면 안되나요? (민서) -> Q 값은 양수 범위에서만 존재하지 않기 때문에 잘못된 값으로 값을 낮출 수 있다. 
     - 🌟 net의 input / output의 크기는 고정이다 !! 🌟
  2. Env 같은 타일을 눌렀을 때 -의 보상을 준다. 
     - 추가적인 룰 없이, 강화학습 방법론만을 사용해 해결하는 방법
