---
title: "[강화시스터즈 1기] 머신러닝 • 강화학습 개론"
author: 이지민
tags: [24-1]
pages: papers
style: border  # fill / border 
color: dark
description: " "
---

# 1주차 강화학습 세션

## 요약

OT 이후 첫 세션을 가졌습니다.😎 

강화학습 이론을 자세히 다루는 것이 중요하다고 생각해 코드 구현없이, 기초적인 개념을 가져가는 것을 메인으로 세션을 진행했습니다. 

구체적인 개인 발표 내용은 추후 **[개념정리] 카테고리**에서 확인하실 수 있습니다.  

## 개인 발표

- 📗 **(컴퓨터 구조) 튜링 머신 vs 폰 노이만 구조** : 변지은 벗
- 📗 **확률 변수와 기댓값이란?** : 이은나 벗
- 📗 **조건부 확률** : 김도희 벗

첫 발표 주자임에도 불구하고, 멋지게 발표를 진행해주셨습니다! 

튜링 머신과 폰 노이만 구조 발표를 통해 컴퓨터의 아이디어와 프로그래밍이 무엇인가에 대해 알 수 있었습니다. 구체적인 예시를 통해 튜링 머신이 어떤 논리구조로 전개되는 지를 알 수 있었습니다. 확률 변수와 기댓값,조건부 확률 발표를 통해 강화학습 기초에 자주 등장하는 개념을 이해할 수 있었습니다. 

## 강화학습 세션

- 📗 **머신러닝 개론**
- 📗 **강화학습 개론**

인공지능부터 강화학습까지 내려오는 계보를 공부했습니다. 인공지능, 머신 러닝, 딥러닝, 강화학습이 정확히 무엇인지 틀을 잡는 시간이었습니다. 

강화학습 개론을 통해, 강화학습의 기본적인 골격을 학습했습니다. 에피소드, 보상과 가치, 상태와 행동, MDP, 정책, 할인과 같이 기초지만 중요한 강화학습의 용어들에 익숙해지는 시간을 가졌습니다. MDP 문제를 정의하고, 이를 기반으로 가치를 계산하는 벨만 방정식을 직접 유도하며, 수학적 엄밀성을 높였습니다. 

## 질의응답

1. 정책은 Greedy하게 값을 선택하는데 어떻게 최대 보상이라는 목표로 수렴할 수 있나요? 
    
    A. 정책은 가치 함수를 기반으로 행동을 선택합니다. 그리고 가치의 기준은 현 상태 이후의 보상의 총합을 반환값으로 갖습니다. 따라서 가치를 기준으로 탐욕적인 선택을 하는 것은 궁극적으로 최대 보상이라는 목표로 수렴을 가능케 합니다. 
    